{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9K7S1j3FOoi0"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet langchain-community langchain-experimental langchain-openai neo4j graphdatascience tiktoken retry"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "0rrFvqkEO9bF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Neo4j in Colab\n",
        "!apt update\n",
        "!apt install openjdk-11-jdk\n",
        "!wget -O - https://debian.neo4j.com/neotechnology.gpg.key | apt-key add -\n",
        "!echo 'deb https://debian.neo4j.com stable 4.4' | tee /etc/apt/sources.list.d/neo4j.list\n",
        "!apt update\n",
        "!apt install neo4j\n",
        "\n",
        "# Start Neo4j\n",
        "!neo4j start\n",
        "\n",
        "# Then connect using\n",
        "from neo4j import GraphDatabase\n",
        "driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"neo4j\"))"
      ],
      "metadata": {
        "id": "qgo2Hhl94zwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GraphRAG with Neo4j and LangChain: Constructing the Graph\n",
        "## Combine text extraction, network analysis, and LLM prompting and summarization for improved RAG accuracy\n",
        "\n",
        "I am always intrigued by new approaches to implementing Retrieval-Augmented Generation (RAG) over graphs, often called GraphRAG. However, it seems that everyone has a different implementation in mind when they hear the term GraphRAG. In this blog post, we will dive deep into the “From Local to Global GraphRAG” article and implementation by Microsoft researchers. We will cover the knowledge graph construction and summarization part and leave the retrievers for the next blog post. The researchers were so kind as to provide us with the code repository, and they have a [project page](https://www.microsoft.com/en-us/research/project/graphrag/) as well.\n",
        "\n",
        "The approach taken in the article mentioned above is quite interesting. As far as I understand, it involves using a knowledge graph as a step in the pipeline for condensing and combining information from multiple sources. Extracting entities and relationships from text is nothing new. However, the authors introduce a novel (at least to me) idea of summarizing condensed graph structure and information back as natural language text. The pipeline begins with input text from documents, which are processed to generate a graph. The graph is then converted back into natural language text, where the generated text contains condensed information about specific entities or graph communities previously spread across multiple documents.\n",
        "\n",
        "![title](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NWkjGfKvWyKJsXfj-HxVjA.png)\n",
        "\n",
        "At a very high level, the input to the GraphRAG pipeline are source documents containing various information. The documents are processed using an LLM to extract structured information about entities appearing in the papers along with their relationships. This extracted structured information is then used to construct a knowledge graph.\n",
        "\n",
        "The advantage of using a knowledge graph data representation is that it can quickly and straightforwardly combine information from multiple documents or data sources about particular entities. As mentioned, the knowledge graph is not the only data representation, though. After the knowledge graph has been constructed, they use a combination of graph algorithms and LLM prompting to generate natural language summaries of communities of entities found in the knowledge graph.\n",
        "\n",
        "These summaries then contain condensed information spreading across multiple data sources and documents for particular entities and communities.\n",
        "\n",
        "For a more detailed understanding of the pipeline, we can refer to the step-by-step description provided in the original paper.\n",
        "\n",
        "![title](https://miro.medium.com/v2/resize:fit:942/format:webp/1*xn10XHns9xQ1WLQxEaSA7w.png)\n",
        "\n",
        "Following is a high-level summary of the pipeline that we will use to reproduce their approach using Neo4j and LangChain.\n",
        "\n",
        "* Indexing — Graph Generation\n",
        "Source Documents to Text Chunks: Source documents are split into smaller text chunks for processing.\n",
        "* Text Chunks to Element Instances: Each text chunk is analyzed to extract entities and relationships, producing a list of tuples representing these elements.\n",
        "* Element Instances to Element Summaries: Extracted entities and relationships are summarized by the LLM into descriptive text blocks for each element.\n",
        "* Element Summaries to Graph Communities: These entity summaries form a graph, which is then partitioned into communities using algorithms like Leiden for hierarchical structure.\n",
        "* Graph Communities to Community Summaries: Summaries of each community are generated with the LLM to understand the dataset’s global topical structure and semantics.\n",
        "\n",
        "#### Retrieval — Answering\n",
        "\n",
        "* Community Summaries to Global Answers: Community summaries are used to answer a user query by generating intermediate answers, which are then aggregated into a final global answer.\n",
        "Note that my implementation was done before their code was available, so there might be slight differences in the underlying approach or LLM prompts being used. I’ll try to explain those differences as we go along.\n",
        "\n",
        "### Setting Up the Neo4j Environment\n",
        "We will use Neo4j as the underlying graph store. The easiest way to get started is to use a free instance of Neo4j Sandbox, which offers cloud instances of the Neo4j database with the Graph Data Science plugin installed. Alternatively, you can set up a local instance of the Neo4j database by downloading the Neo4j Desktop application and creating a local database instance. If you are using a local version, make sure to install both APOC and GDS plugins. For production setups, you can use the paid, managed AuraDS (Data Science) instance, which provides the GDS plugin.\n",
        "\n",
        "We start by creating a Neo4jGraph instance, which is the convenience wrapper we added to LangChain:"
      ],
      "metadata": {
        "id": "dmnDlh4BeFJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative: Modify Neo4j config to disable authentication temporarily\n",
        "def setup_neo4j_no_auth():\n",
        "    import subprocess\n",
        "    import time\n",
        "    import os\n",
        "\n",
        "    try:\n",
        "        # Stop Neo4j\n",
        "        subprocess.run(['neo4j', 'stop'], capture_output=True)\n",
        "        time.sleep(5)\n",
        "\n",
        "        # Disable authentication temporarily\n",
        "        config_file = \"/etc/neo4j/neo4j.conf\"\n",
        "        with open(config_file, 'a') as f:\n",
        "            f.write(\"\\ndbms.security.auth_enabled=false\\n\")\n",
        "\n",
        "        # Start Neo4j\n",
        "        subprocess.run(['neo4j', 'start'], check=True)\n",
        "        time.sleep(15)\n",
        "\n",
        "        # Set environment variables (no password needed)\n",
        "        os.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"\n",
        "        os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
        "        os.environ[\"NEO4J_PASSWORD\"] = \"\"\n",
        "\n",
        "        print(\"Neo4j setup complete with authentication disabled!\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return False\n",
        "\n",
        "# Try this if the first method fails\n",
        "setup_neo4j_no_auth()"
      ],
      "metadata": {
        "id": "73QU_TL0CQQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset\n",
        "We will use a news article dataset I created some time ago using Diffbot’s API. I have uploaded it to my GitHub for easier reuse:"
      ],
      "metadata": {
        "id": "C2BE07dRfHf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tiktoken\n",
        "\n",
        "\n",
        "def num_tokens_from_string(string: str, model: str = \"gpt-4o\") -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "\n",
        "news = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/news_articles.csv\"\n",
        ")\n",
        "news[\"tokens\"] = [\n",
        "    num_tokens_from_string(f\"{row['title']} {row['text']}\")\n",
        "    for i, row in news.iterrows()\n",
        "]\n",
        "news.head()"
      ],
      "metadata": {
        "id": "VKwjKqPyP78d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the title and text of the articles available, along with their publishing date and token count using the tiktoken library.\n",
        "\n",
        "### Text Chunking\n",
        "The text chunking step is crucial and significantly impacts downstream results. The paper authors found that using smaller text chunks results in extracting more entities overall.\n",
        "\n",
        "![title](https://miro.medium.com/v2/resize:fit:1120/format:webp/1*9HdF1xQ6Tm6dazOQBFvSvg.png)\n",
        "\n",
        "As you can see, using text chunks of 2,400 tokens results in fewer extracted entities than when they used 600 tokens. Additionally, they identified that LLMs might not extract all entities on the first run. In that case, they introduce a heuristics to perform the extraction multiple times. We will talk about that more in the next section.\n",
        "\n",
        "However, there are always trade-offs. Using smaller text chunks can result in losing the context and coreferences of specific entities spread across the documents. For example, if a document mentions “John” and “he” in separate sentences, breaking the text into smaller chunks might make it unclear that “he” refers to John. Some of the coreference issues can be solved using an overlap text chunking strategy, but not all of them.\n",
        "\n",
        "Let’s examine the size of our article texts:"
      ],
      "metadata": {
        "id": "Z_nPbFIFfMqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "sns.histplot(news[\"tokens\"], kde=False)\n",
        "plt.title('Distribution of chunk sizes')\n",
        "plt.xlabel('Token count')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ebzDWs-io-Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution of article token counts is approximately normal, with a peak of around 400 tokens. The frequency of chunks gradually increases up to this peak, then decreases symmetrically, indicating most text chunks are near the 400-token mark.\n",
        "\n",
        "Due to this distribution, we will not perform any text chunking here to avoid coreference issues. By default, the GraphRAG project uses chunk sizes of 300 tokens with 100 tokens of overlap.\n",
        "\n",
        "### Extracting Nodes and Relationships\n",
        "The next step is constructing knowledge from text chunks. For this use case, we use an LLM to extract structured information in the form of nodes and relationships from the text. You can examine the LLM prompt the authors used in the paper. They have LLM prompts where we can predefine node labels if needed, but by default, that’s optional. Additionally, the extracted relationships in the original documentation don’t really have a type, only a description. I imagine the reason behind this choice is to allow the LLM to extract and retain richer and more nuanced information as relationships. But it’s difficult to have a clean knowledge graph with no relationship-type specifications (the descriptions could go into a property).\n",
        "\n",
        "In our implementation, we will use the LLMGraphTransformer, which is available in the LangChain library. Instead of using pure prompt engineering, as the implementation in the article paper does, the LLMGraphTransformer uses the built-in function calling support to extract structured information (structured output LLMs in LangChain). You can inspect the system prompt:"
      ],
      "metadata": {
        "id": "-tBXEXvRjQVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
        "\n",
        "llm_transformer = LLMGraphTransformer(\n",
        "    llm=llm,\n",
        "    node_properties=[\"description\"],\n",
        "    relationship_properties=[\"description\"]\n",
        ")"
      ],
      "metadata": {
        "id": "INfxdQRCPxyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from langchain_community.graphs.graph_document import GraphDocument\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "def process_text(text: str) -> List[GraphDocument]:\n",
        "    doc = Document(page_content=text)\n",
        "    return llm_transformer.convert_to_graph_documents([doc])"
      ],
      "metadata": {
        "id": "doaut5qvTr36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we use GPT-4o for graph extraction. The authors specifically instruct the LLM to extract entities and relationships and their descriptions. With the LangChain implementation, you can use the node_propertiesand relationship_propertiesattributes to specify which node or relationship properties you want the LLM to extract.\n",
        "\n",
        "The difference with the LLMGraphTransformer implementation is that all node or relationship properties are optional, so not all nodes will have the descriptionproperty. If we wanted, we could define a custom extraction to have a mandatory descriptionproperty, but we will skip that in this implementation.\n",
        "\n",
        "We will parallelize the requests to make the graph extraction faster and store results to Neo4j:"
      ],
      "metadata": {
        "id": "WsdJJlNpfjuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "\n",
        "MAX_WORKERS = 10\n",
        "NUM_ARTICLES = 2000\n",
        "graph_documents = []\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "    # Submitting all tasks and creating a list of future objects\n",
        "    futures = [\n",
        "        executor.submit(process_text, f\"{row['title']} {row['text']}\")\n",
        "        for i, row in news.head(NUM_ARTICLES).iterrows()\n",
        "    ]\n",
        "\n",
        "    for future in tqdm(\n",
        "        as_completed(futures), total=len(futures), desc=\"Processing documents\"\n",
        "    ):\n",
        "        graph_document = future.result()\n",
        "        graph_documents.extend(graph_document)\n",
        "\n",
        "graph.add_graph_documents(\n",
        "    graph_documents,\n",
        "    baseEntityLabel=True,\n",
        "    include_source=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "vy9GaXqXRrQu",
        "outputId": "fd916248-a705-4478-c8b3-e0221520d8a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing documents:   0%|          | 0/2000 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-b246caa71260>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mas_completed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Processing documents\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     ):\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mgraph_document\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mgraph_documents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_document\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-e41913421889>\u001b[0m in \u001b[0;36mprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGraphDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mllm_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_graph_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_experimental/graph_transformers/llm.py\u001b[0m in \u001b[0;36mconvert_to_graph_documents\u001b[0;34m(self, documents, config)\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGraphDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mtransformed\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgraphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \"\"\"\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     async def aprocess_response(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_experimental/graph_transformers/llm.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGraphDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mtransformed\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgraphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \"\"\"\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     async def aprocess_response(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_experimental/graph_transformers/llm.py\u001b[0m in \u001b[0;36mprocess_response\u001b[0;34m(self, document, config)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \"\"\"\n\u001b[1;32m    838\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m         \u001b[0mraw_schema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_call\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m             \u001b[0mraw_schema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_schema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3045\u001b[0m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3046\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3047\u001b[0;31m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3048\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3772\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3773\u001b[0m                 ]\n\u001b[0;32m-> 3774\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3775\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3776\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3772\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3773\u001b[0m                 ]\n\u001b[0;32m-> 3774\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3775\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3776\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_invoke_step\u001b[0;34m(step, input_, config, key)\u001b[0m\n\u001b[1;32m   3756\u001b[0m             )\n\u001b[1;32m   3757\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_config\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3758\u001b[0;31m                 return context.run(\n\u001b[0m\u001b[1;32m   3759\u001b[0m                     \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3760\u001b[0m                     \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5428\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5429\u001b[0m     ) -> Output:\n\u001b[0;32m-> 5430\u001b[0;31m         return self.bound.invoke(\n\u001b[0m\u001b[1;32m   5431\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5432\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m         return cast(\n\u001b[1;32m    371\u001b[0m             \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    373\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    955\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    956\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m                 results.append(\n\u001b[0;32m--> 776\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    777\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1020\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1023\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[0mpayload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompletions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBadRequestError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m                 \u001b[0m_handle_openai_bad_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/beta/chat/completions.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    156\u001b[0m             )\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m         )\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we extract graph information from 2,000 articles and store results to Neo4j. We have extracted around 13,000 entities and 16,000 relationships. Here is an example of an extracted document in the graph.\n",
        "\n",
        "\n",
        "![title](https://miro.medium.com/v2/resize:fit:952/format:webp/1*pj-V-XHmVaJE5SsOtvqOqw.png)\n",
        "\n",
        "It takes about 35 (+/- 5) minutes to complete extraction and costs about $30 with GPT-4o.\n",
        "\n",
        "In this step, the authors introduce heuristics to decide whether to extract graph information in more than one pass. For simplicity’s sake, we will only do one pass. However, if we wanted to do multiple passes, we could put the first extraction results as conversational history and simply [instruct the LLM that many entities are missing](https://github.com/microsoft/graphrag/blob/main/graphrag/index/graph/extractors/claims/prompts.py#L60), and it should extract more, like the GraphRAG authors do.\n",
        "\n",
        "Previously, I mentioned how vital text chunk size is and how it affects the number of entities extracted. Since we didn’t perform any additional text chunking, we can evaluate the distribution of extracted entities based on text chunk size:"
      ],
      "metadata": {
        "id": "iPqW-TpOftgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entity_dist = graph.query(\n",
        "    \"\"\"\n",
        "MATCH (d:Document)\n",
        "RETURN d.text AS text,\n",
        "       count {(d)-[:MENTIONS]->()} AS entity_count\n",
        "\"\"\"\n",
        ")\n",
        "entity_dist_df = pd.DataFrame.from_records(entity_dist)\n",
        "entity_dist_df[\"token_count\"] = [\n",
        "    num_tokens_from_string(str(el)) for el in entity_dist_df[\"text\"]\n",
        "]\n",
        "# Scatter plot with regression line\n",
        "sns.lmplot(\n",
        "    x=\"token_count\", y=\"entity_count\", data=entity_dist_df, line_kws={\"color\": \"red\"}\n",
        ")\n",
        "plt.title(\"Entity Count vs Token Count Distribution\")\n",
        "plt.xlabel(\"Token Count\")\n",
        "plt.ylabel(\"Entity Count\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "u2Qz8jYbpd8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scatter plot shows that while there is a positive trend, indicated by the red line, the relationship is sublinear. Most data points cluster at lower entity counts, even as token counts increase. This indicates that the number of entities extracted does not scale proportionally with the size of the text chunks. Although some outliers exist, the general pattern shows that higher token counts do not consistently lead to higher entity counts. This validates the authors’ finding that lower text chunk sizes will extract more information.\n",
        "\n",
        "I also thought it would be interesting to inspect the node degree distributions of the constructed graph. The following code retrieves and visualizes node degree distributions:"
      ],
      "metadata": {
        "id": "VyEy_AgFgDj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "degree_dist = graph.query(\n",
        "    \"\"\"\n",
        "MATCH (e:__Entity__)\n",
        "RETURN count {(e)-[:!MENTIONS]-()} AS node_degree\n",
        "\"\"\"\n",
        ")\n",
        "degree_dist_df = pd.DataFrame.from_records(degree_dist)\n",
        "\n",
        "# Calculate mean and median\n",
        "mean_degree = np.mean(degree_dist_df['node_degree'])\n",
        "percentiles = np.percentile(degree_dist_df['node_degree'], [25, 50, 75, 90])\n",
        "# Create a histogram with a logarithmic scale\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(degree_dist_df['node_degree'], bins=50, kde=False, color='blue')\n",
        "# Use a logarithmic scale for the x-axis\n",
        "plt.yscale('log')\n",
        "# Adding labels and title\n",
        "plt.xlabel('Node Degree')\n",
        "plt.ylabel('Count (log scale)')\n",
        "plt.title('Node Degree Distribution')\n",
        "# Add mean, median, and percentile lines\n",
        "plt.axvline(mean_degree, color='red', linestyle='dashed', linewidth=1, label=f'Mean: {mean_degree:.2f}')\n",
        "plt.axvline(percentiles[0], color='purple', linestyle='dashed', linewidth=1, label=f'25th Percentile: {percentiles[0]:.2f}')\n",
        "plt.axvline(percentiles[1], color='orange', linestyle='dashed', linewidth=1, label=f'50th Percentile: {percentiles[1]:.2f}')\n",
        "plt.axvline(percentiles[2], color='yellow', linestyle='dashed', linewidth=1, label=f'75th Percentile: {percentiles[2]:.2f}')\n",
        "plt.axvline(percentiles[3], color='brown', linestyle='dashed', linewidth=1, label=f'90th Percentile: {percentiles[3]:.2f}')\n",
        "# Add legend\n",
        "plt.legend()\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eYhQaUkQOGyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The node degree distribution follows a power-law pattern, indicating most nodes have very few connections while a few nodes are highly connected. The mean degree is 2.45, and the median is 1.00, showing that more than half the nodes have only one connection. Most nodes (75 percent) have two or fewer connections, and 90 percent have five or fewer. This distribution is typical of many real-world networks, where a small number of hubs have many connections, and most nodes have few.\n",
        "\n",
        "Since both node and relationship descriptions are not mandatory properties, we will also examine how many were extracted:"
      ],
      "metadata": {
        "id": "eE3oNAl6gIHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph.query(\"\"\"\n",
        "MATCH (n:`__Entity__`)\n",
        "RETURN \"node\" AS type,\n",
        "       count(*) AS total_count,\n",
        "       count(n.description) AS non_null_descriptions\n",
        "UNION ALL\n",
        "MATCH (n)-[r:!MENTIONS]->()\n",
        "RETURN \"relationship\" AS type,\n",
        "       count(*) AS total_count,\n",
        "       count(r.description) AS non_null_descriptions\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "yhOk0M-6a-a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results show that 5,926 nodes out of 12,994 (45.6 percent) have the description property. On the other hand, only 5,569 relationships out of 15,921 (35 percent) have such a property.\n",
        "\n",
        "Note that due to the probabilistic nature of LLMs, the numbers can vary on different runs and different source data, LLMs, and prompts.\n",
        "\n",
        "### Entity Resolution\n",
        "Entity resolution (de-duplication) is crucial when constructing knowledge graphs because it ensures that each entity is uniquely and accurately represented, preventing duplicates and merging records that refer to the same real-world entity. This process is essential for maintaining data integrity and consistency within the graph. Without entity resolution, knowledge graphs would suffer from fragmented and inconsistent data, leading to errors and unreliable insights.\n",
        "\n",
        "![title](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rlWiW1sWlixDTaY0.png)\n",
        "\n",
        "This image demonstrates how a single real-world entity might appear under slightly different names in different documents and, consequently, in our graph.\n",
        "\n",
        "Moreover, sparse data becomes a significant issue without entity resolution. Incomplete or partial data from various sources can result in scattered and disconnected pieces of information, making it difficult to form a coherent and comprehensive understanding of entities. Accurate entity resolution addresses this by consolidating data, filling in gaps, and creating a unified view of each entity.\n",
        "\n",
        "Overall, entity resolution enhances the efficiency of data retrieval and integration, providing a cohesive view of information across different sources. It ultimately enables more effective question-answering based on a reliable and complete knowledge graph.\n",
        "\n",
        "Unfortunately, the authors of the GraphRAG paper did not include any entity resolution code in their repo, although they mention it in their paper. One reason for leaving this code out could be that it is tough to implement a robust and well-performing entity resolution for any given domain. You can implement custom heuristics for different nodes when dealing with pre-defined types of nodes (when they aren’t predefined, they aren’t consistent enough, like company, organization, business, etc.). However, if the node labels or types aren’t known in advance, as in our case, this becomes an even harder problem. Nonetheless, we will implement a version of entity resolution in our project here, combining text embeddings and graph algorithms with word distance and LLMs.\n",
        "\n",
        "![title](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KlLF01Imn7RlyuSWYkwnqw.png)\n",
        "\n",
        "Our process for entity resolution involves the following steps:\n",
        "\n",
        "1. Entities in the graph — Start with all entities within the graph.\n",
        "2. K-nearest graph — Construct a k-nearest neighbor graph, connecting similar entities based on text embeddings.\n",
        "3. Weakly Connected Components — Identify weakly connected components in the k-nearest graph, grouping entities that are likely to be similar. Add a word distance filtering step after these components have been identified.\n",
        "4. LLM evaluation — Use an LLM to evaluate these components and decide whether the entities within each component should be merged, resulting in a final decision on entity resolution (for example, merging ‘Silicon Valley Bank’ and ‘Silicon_Valley_Bank’ while rejecting the merge for different dates like ‘September 16, 2023’ and ‘September 2, 2023’).\n",
        "\n",
        "We begin by calculating text embeddings for the name and description properties of entities. We can use the from_existing_graph method in the Neo4jVector integration in LangChain to achieve this:"
      ],
      "metadata": {
        "id": "FVphl4p5jIZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Neo4jVector\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "vector = Neo4jVector.from_existing_graph(\n",
        "    OpenAIEmbeddings(),\n",
        "    node_label='__Entity__',\n",
        "    text_node_properties=['id', 'description'],\n",
        "    embedding_node_property='embedding'\n",
        ")"
      ],
      "metadata": {
        "id": "LYkKe0XOWPTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use these embeddings to find potential candidates that are similar based on the cosine distance of these embeddings. We will use graph algorithms available in the Graph Data Science (GDS) library; therefore, we can use the GDS Python client for ease of use in a Pythonic way:"
      ],
      "metadata": {
        "id": "9wvxowyhgk5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from graphdatascience import GraphDataScience\n",
        "# project graph\n",
        "\n",
        "gds = GraphDataScience(\n",
        "    os.environ[\"NEO4J_URI\"],\n",
        "    auth=(os.environ[\"NEO4J_USERNAME\"], os.environ[\"NEO4J_PASSWORD\"])\n",
        ")"
      ],
      "metadata": {
        "id": "3L4VqsAmYspT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are not familiar with the GDS library, we first have to project an in-memory graph before we can execute any graph algorithms.\n",
        "\n",
        "![title](https://miro.medium.com/v2/resize:fit:1268/format:webp/1*YdEljYtG2ddd7DrrlzS6yg.png)\n",
        "\n",
        "First, the Neo4j stored graph is projected into an in-memory graph for faster processing and analysis. Next, a graph algorithm is executed on the in-memory graph. Optionally, the algorithm’s results can be stored back into the Neo4j database. Learn more about it in the documentation.\n",
        "\n",
        "To create the k-nearest neighbor graph, we will project all entities along with their text embeddings:"
      ],
      "metadata": {
        "id": "UGy3e-g6gl-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "G, result = gds.graph.project(\n",
        "    \"entities\",                   #  Graph name\n",
        "    \"__Entity__\",                 #  Node projection\n",
        "    \"*\",                          #  Relationship projection\n",
        "    nodeProperties=[\"embedding\"]  #  Configuration parameters\n",
        ")"
      ],
      "metadata": {
        "id": "nJpLldiUdpsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the graph is projected under the entities name, we can execute graph algorithms. We will begin by constructing a k-nearest graph. The two most important parameters influencing how sparse or dense the k-nearest graph will be are similarityCutoff and topK. The topKis the number of neighbors to find for each node, with a minimum value of 1. The similarity cutoff filters out relationships with similarity below this threshold. Here, we will use a default topKof 10 and a relatively high similarity cutoff of 0.95. Using a high similarity cutoff, such as 0.95, ensures that only highly similar pairs are considered matches, minimizing false positives and improving accuracy.\n",
        "\n",
        "![title](https://miro.medium.com/v2/resize:fit:942/format:webp/1*Wt_9Tj9P_FsU7w1IvpO39g.png)\n",
        "\n",
        "Since we want to store the results back to the projected in-memory graph instead of the knowledge graph, we will use the mutate mode of the algorithm:"
      ],
      "metadata": {
        "id": "Fthtqjo-gw2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_threshold = 0.95\n",
        "\n",
        "gds.knn.mutate(\n",
        "  G,\n",
        "  nodeProperties=['embedding'],\n",
        "  mutateRelationshipType= 'SIMILAR',\n",
        "  mutateProperty= 'score',\n",
        "  similarityCutoff=similarity_threshold\n",
        ")"
      ],
      "metadata": {
        "id": "DWdGhMmgeuFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to identify groups of entities that are connected with the newly inferred similarity relationships. Identifying groups of connected nodes is a frequent process in network analysis, often called community detection or clustering, which involves finding subgroups of densely connected nodes. In this example, we will use the Weakly Connected Components algorithm, which helps us find parts of a graph where all nodes are connected, even if we ignore the direction of the connections.\n",
        "\n",
        "![title](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZhawRXqvoZ6EuuUzacCf7A.png)\n",
        "\n",
        "We use the algorithm’s write mode to store the results back to the database (stored graph):"
      ],
      "metadata": {
        "id": "-Dg7vZrGg7ZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gds.wcc.write(\n",
        "    G,\n",
        "    writeProperty=\"wcc\",\n",
        "    relationshipTypes=[\"SIMILAR\"]\n",
        ")"
      ],
      "metadata": {
        "id": "Juc1fFiFf73-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text embedding comparison helps find potential duplicates, but it is only part of the entity resolution process. For example, Google and Apple are very close in the embedding space (0.96 cosine similarity using the ada-002 embedding model). The same goes for BMW and Mercedes Benz (0.97 cosine similarity). High text embedding similarity is a good start, but we can improve it. Therefore, we will add an additional filter allowing only pairs of words with a text distance of three or fewer (meaning that only the characters can be changed):"
      ],
      "metadata": {
        "id": "V8euMSB4hElG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_edit_distance = 3\n",
        "potential_duplicate_candidates = graph.query(\n",
        "    \"\"\"MATCH (e:`__Entity__`)\n",
        "    WHERE size(e.id) > 4 // longer than 4 characters\n",
        "    WITH e.wcc AS community, collect(e) AS nodes, count(*) AS count\n",
        "    WHERE count > 1\n",
        "    UNWIND nodes AS node\n",
        "    // Add text distance\n",
        "    WITH distinct\n",
        "      [n IN nodes WHERE apoc.text.distance(toLower(node.id), toLower(n.id)) < $distance | n.id] AS intermediate_results\n",
        "    WHERE size(intermediate_results) > 1\n",
        "    WITH collect(intermediate_results) AS results\n",
        "    // combine groups together if they share elements\n",
        "    UNWIND range(0, size(results)-1, 1) as index\n",
        "    WITH results, index, results[index] as result\n",
        "    WITH apoc.coll.sort(reduce(acc = result, index2 IN range(0, size(results)-1, 1) |\n",
        "            CASE WHEN index <> index2 AND\n",
        "                size(apoc.coll.intersection(acc, results[index2])) > 0\n",
        "                THEN apoc.coll.union(acc, results[index2])\n",
        "                ELSE acc\n",
        "            END\n",
        "    )) as combinedResult\n",
        "    WITH distinct(combinedResult) as combinedResult\n",
        "    // extra filtering\n",
        "    WITH collect(combinedResult) as allCombinedResults\n",
        "    UNWIND range(0, size(allCombinedResults)-1, 1) as combinedResultIndex\n",
        "    WITH allCombinedResults[combinedResultIndex] as combinedResult, combinedResultIndex, allCombinedResults\n",
        "    WHERE NOT any(x IN range(0,size(allCombinedResults)-1,1)\n",
        "        WHERE x <> combinedResultIndex\n",
        "        AND apoc.coll.containsAll(allCombinedResults[x], combinedResult)\n",
        "    )\n",
        "    RETURN combinedResult\n",
        "    \"\"\", params={'distance': word_edit_distance})\n",
        "potential_duplicate_candidates[:5]"
      ],
      "metadata": {
        "id": "S2mjWpcagO-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Cypher statement is slightly more involved, and its interpretation is beyond the scope of this blog post. You can always ask an LLM to interpret it.\n",
        "\n",
        "Additionally, the word distance cutoff could be a function of the length of the word instead of a single number and the implementation could be more scalable.\n",
        "\n",
        "What is important is that it outputs groups of potential entities we might want to merge.\n",
        "\n",
        "As you can see, our resolution approach works better for some node types than others. Based on a quick examination, it seems to work better for people and organizations, while it’s pretty bad for dates. If we used predefined node types, we could prepare different heuristics for various node types. In this example, we do not have predefined node labels, so we will turn to an LLM to make the final decision about whether entities should be merged or not.\n",
        "\n",
        "First, we need to formulate the LLM prompt to effectively guide and inform the final decision regarding the merging of the nodes:"
      ],
      "metadata": {
        "id": "0X-UwzLqhJT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"You are a data processing assistant. Your task is to identify duplicate entities in a list and decide which of them should be merged.\n",
        "The entities might be slightly different in format or content, but essentially refer to the same thing. Use your analytical skills to determine duplicates.\n",
        "\n",
        "Here are the rules for identifying duplicates:\n",
        "1. Entities with minor typographical differences should be considered duplicates.\n",
        "2. Entities with different formats but the same content should be considered duplicates.\n",
        "3. Entities that refer to the same real-world object or concept, even if described differently, should be considered duplicates.\n",
        "4. If it refers to different numbers, dates, or products, do not merge results\n",
        "\"\"\"\n",
        "user_template = \"\"\"\n",
        "Here is the list of entities to process:\n",
        "{entities}\n",
        "\n",
        "Please identify duplicates, merge them, and provide the merged list.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ABQOLHRshVU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I always like to use with_structured_output method in LangChain when expecting structured data output to avoid having to parse the outputs manually.\n",
        "\n",
        "Here, we will define the output as a list of lists, where each inner list contains the entities that should be merged. This structure is used to handle scenarios where, for example, the input might be [Sony, Sony Inc, Google, Google Inc]. In such cases, you would want to merge “Sony” and “Sony Inc” separately from “Google” and “Google Inc.”"
      ],
      "metadata": {
        "id": "1CbHzuTohXzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from typing import List, Optional\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from retry import retry\n",
        "\n",
        "class DuplicateEntities(BaseModel):\n",
        "    entities: List[str] = Field(\n",
        "        description=\"Entities that represent the same object or real-world entity and should be merged\"\n",
        "    )\n",
        "\n",
        "\n",
        "class Disambiguate(BaseModel):\n",
        "    merge_entities: Optional[List[DuplicateEntities]] = Field(\n",
        "        description=\"Lists of entities that represent the same object or real-world entity and should be merged\"\n",
        "    )\n",
        "\n",
        "\n",
        "extraction_llm = ChatOpenAI(model_name=\"gpt-4o\").with_structured_output(\n",
        "    Disambiguate\n",
        ")\n",
        "\n",
        "extraction_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            system_prompt,\n",
        "        ),\n",
        "        (\n",
        "            \"human\",\n",
        "            user_template,\n",
        "        ),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "f3uaBpUEF3EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we integrate the LLM prompt with the structured output to create a chain using LangChain Expression Language (LCEL) syntax and encapsulate it within a disambiguate function."
      ],
      "metadata": {
        "id": "QWFkbPsthcTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_chain = extraction_prompt | extraction_llm\n",
        "\n",
        "@retry(tries=3, delay=2)\n",
        "def entity_resolution(entities: List[str]) -> Optional[List[str]]:\n",
        "    return [\n",
        "        el.entities\n",
        "        for el in extraction_chain.invoke({\"entities\": entities}).merge_entities\n",
        "    ]\n",
        "\n"
      ],
      "metadata": {
        "id": "rRyqLAHShguX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entity_resolution(['Star Ocean The Second Story R', 'Star Ocean: The Second Story R'])"
      ],
      "metadata": {
        "id": "cPMwVQjIJQAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entity_resolution({\"entities\": ['December 16, 2023',\n",
        "   'December 2, 2023',\n",
        "   'December 23, 2023',\n",
        "   'December 26, 2023',\n",
        "   'December 30, 2023',\n",
        "   'December 5, 2023',\n",
        "   'December 9, 2023']})"
      ],
      "metadata": {
        "id": "u3gNrAoUJdKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to run all potential candidate nodes through the entity_resolution function to decide whether they should be merged. To speed up the process, we will again parallelize the LLM calls:"
      ],
      "metadata": {
        "id": "jL0WhRpChpDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_entities = []\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "    # Submitting all tasks and creating a list of future objects\n",
        "    futures = [\n",
        "        executor.submit(entity_resolution, el['combinedResult'])\n",
        "        for el in potential_duplicate_candidates\n",
        "    ]\n",
        "\n",
        "    for future in tqdm(\n",
        "        as_completed(futures), total=len(futures), desc=\"Processing documents\"\n",
        "    ):\n",
        "        to_merge = future.result()\n",
        "        if to_merge:\n",
        "            merged_entities.extend(to_merge)"
      ],
      "metadata": {
        "id": "ofXSUcm8KSrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_entities[:10]"
      ],
      "metadata": {
        "id": "OmuqZmrGLmSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final step of entity resolution involves taking the results from the entity_resolution LLM and writing them back to the database by merging the specified nodes:\n",
        "\n"
      ],
      "metadata": {
        "id": "peZElkVJhuaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph.query(\"\"\"\n",
        "UNWIND $data AS candidates\n",
        "CALL {\n",
        "  WITH candidates\n",
        "  MATCH (e:__Entity__) WHERE e.id IN candidates\n",
        "  RETURN collect(e) AS nodes\n",
        "}\n",
        "CALL apoc.refactor.mergeNodes(nodes, {properties: {\n",
        "    `.*`: 'discard'\n",
        "}})\n",
        "YIELD node\n",
        "RETURN count(*)\n",
        "\"\"\", params={\"data\": merged_entities})"
      ],
      "metadata": {
        "id": "e8Mg310yMLhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G.drop()"
      ],
      "metadata": {
        "id": "XgGHDn-Wi1BH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This entity resolution is not perfect, but it gives us a starting point upon which we can improve. Additionally, we can improve the logic for determining which entities should be retained.\n",
        "\n",
        "### Element Summarization\n",
        "In the next step, the authors perform an element summarization step. Essentially, every node and relationship gets passed through an entity summarization prompt. The authors note the novelty and interest of their approach:\n",
        "\n",
        "“Overall, our use of rich descriptive text for homogeneous nodes in a potentially noisy graph structure is aligned with both the capabilities of LLMs and the needs of global, query-focused summarization. These qualities also differentiate our graph index from typical knowledge graphs, which rely on concise and consistent knowledge triples (subject, predicate, object) for downstream reasoning tasks.”\n",
        "\n",
        "The idea is exciting. We still extract subject and object IDs or names from text, which allows us to link relationships to correct entities, even when entities appear across multiple text chunks. However, the relationships aren’t reduced to a single type. Instead, the relationship type is actually a freeform text that allows us to retain richer and more nuanced information.\n",
        "\n",
        "Additionally, the entity information is summarized using an LLM, allowing us to embed and index this information and entities more efficiently for more accurate retrieval.\n",
        "\n",
        "One could argue that this richer and more nuanced information could also be retained by adding additional, possibly arbitrary, node and relationship properties. One issue with arbitrary node and relationship properties is that it could be hard to extract the information consistently because the LLM might use different property names or focus on various details on every execution.\n",
        "\n",
        "Some of these problems could be solved using predefined property names with additional type and description information. In that case, you would need a subject-matter expert to help define those properties, leaving little room for an LLM to extract any vital information outside the predefined descriptions.\n",
        "\n",
        "It’s an exciting approach to represent richer information in a knowledge graph.\n",
        "\n",
        "One potential issue with the element summarization step is that it does not scale well since it requires an LLM call for every entity and relationship in the graph. Our graph is relatively tiny with 13,000 nodes and 16,000 relationships. Even for such a small graph, we would require 29,000 LLM calls, and each call would use a couple hundred tokens, making it quite expensive and time-intensive. Therefore, we will avoid this step here. We can still use the description properties extracted during the initial text processing.\n",
        "\n",
        "### Constructing and Summarizing Communities\n",
        "The final step in the graph construction and indexing process involves identifying communities within the graph. In this context, a community is a group of nodes that are more densely connected to each other than to the rest of the graph, indicating a higher level of interaction or similarity. The following visualization shows an example of community detection results.\n",
        "\n",
        "![title](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*l2i3ctl8s6xdR6ul.png)\n",
        "\n",
        "Once these entity communities are identified with a clustering algorithm, an LLM generates a summary for each community, providing insights into their individual characteristics and relationships.\n",
        "\n",
        "Again, we use the Graph Data Science library. We start by projecting an in-memory graph. To follow the original article precisely, we will project the graph of entities as an undirected weighted network, where the network represents the number of connections between two entities:"
      ],
      "metadata": {
        "id": "7Nwk7OgPjVPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "G, result = gds.graph.project(\n",
        "    \"communities\",  #  Graph name\n",
        "    \"__Entity__\",  #  Node projection\n",
        "    {\n",
        "        \"_ALL_\": {\n",
        "            \"type\": \"*\",\n",
        "            \"orientation\": \"UNDIRECTED\",\n",
        "            \"properties\": {\"weight\": {\"property\": \"*\", \"aggregation\": \"COUNT\"}},\n",
        "        }\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "9VCZpBBMjGQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The authors employed the Leiden algorithm, a hierarchical clustering method, to identify communities within the graph. One advantage of using a hierarchical community detection algorithm is the ability to examine communities at multiple levels of granularity. The authors suggest summarizing all communities at each level, providing a comprehensive understanding of the graph’s structure.\n",
        "\n",
        "First, we will use the Weakly Connected Components (WCC) algorithm to assess the connectivity of our graph. This algorithm identifies isolated sections within the graph, meaning it detects subsets of nodes or components that are connected to each other but not to the rest of the graph. These components help us understand the fragmentation within the network and identify groups of nodes that are independent from others. WCC is vital for analyzing the overall structure and connectivity of the graph."
      ],
      "metadata": {
        "id": "-tuPpCJ3iAZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wcc = gds.wcc.stats(G)\n",
        "print(f\"Component count: {wcc['componentCount']}\")\n",
        "print(f\"Component distribution: {wcc['componentDistribution']}\")"
      ],
      "metadata": {
        "id": "Hs-nCOuWuW6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The WCC algorithm results identified 1,119 distinct components. Notably, the largest component comprises 9,109 nodes, common in real-world networks where a single super component coexists with numerous smaller isolated components. The smallest component has one node, and the average component size is about 11.3 nodes.\n",
        "\n",
        "Next, we will run the Leiden algorithm, which is also available in the GDS library, and enable the includeIntermediateCommunities parameter to return and store communities at all levels. We have also included a relationshipWeightProperty parameter to run the weighted variant of the Leiden algorithm. Using the write mode of the algorithm stores the results as a node property."
      ],
      "metadata": {
        "id": "ZbQkCrH8iCPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gds.leiden.write(\n",
        "    G,\n",
        "    writeProperty=\"communities\",\n",
        "    includeIntermediateCommunities=True,\n",
        "    relationshipWeightProperty=\"weight\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "O9aLrUsykFa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The algorithm identified five levels of communities, with the highest (least granular level where communities are largest) having 1,188 communities (as opposed to 1,119 components). Building on this, we will create a distinct node for each community and represent their hierarchical structure as an interconnected graph. Later, we will also store community summaries and other attributes as node properties."
      ],
      "metadata": {
        "id": "0a5AxvzLiHHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph.query(\"CREATE CONSTRAINT IF NOT EXISTS FOR (c:__Community__) REQUIRE c.id IS UNIQUE;\")"
      ],
      "metadata": {
        "id": "-qIC9PZcGujw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph.query(\"\"\"\n",
        "MATCH (e:`__Entity__`)\n",
        "UNWIND range(0, size(e.communities) - 1 , 1) AS index\n",
        "CALL {\n",
        "  WITH e, index\n",
        "  WITH e, index\n",
        "  WHERE index = 0\n",
        "  MERGE (c:`__Community__` {id: toString(index) + '-' + toString(e.communities[index])})\n",
        "  ON CREATE SET c.level = index\n",
        "  MERGE (e)-[:IN_COMMUNITY]->(c)\n",
        "  RETURN count(*) AS count_0\n",
        "}\n",
        "CALL {\n",
        "  WITH e, index\n",
        "  WITH e, index\n",
        "  WHERE index > 0\n",
        "  MERGE (current:`__Community__` {id: toString(index) + '-' + toString(e.communities[index])})\n",
        "  ON CREATE SET current.level = index\n",
        "  MERGE (previous:`__Community__` {id: toString(index - 1) + '-' + toString(e.communities[index - 1])})\n",
        "  ON CREATE SET previous.level = index - 1\n",
        "  MERGE (previous)-[:IN_COMMUNITY]->(current)\n",
        "  RETURN count(*) AS count_1\n",
        "}\n",
        "RETURN count(*)\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "EKRaBUjpwNNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The authors also introduce a community rank, indicating the number of distinct text chunks in which the entities within the community appear:"
      ],
      "metadata": {
        "id": "hoIno_V3iOOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph.query(\"\"\"\n",
        "MATCH (c:__Community__)<-[:IN_COMMUNITY*]-(:__Entity__)<-[:MENTIONS]-(d:Document)\n",
        "WITH c, count(distinct d) AS rank\n",
        "SET c.community_rank = rank;\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "QA4CZxcyIkd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s examine a sample hierarchical structure with many intermediate communities merging at higher levels. The communities are non-overlapping, meaning that each entity belongs to precisely a single community at each level.\n",
        "Let’s examine the number of communities and their sizes and different levels in more detail:"
      ],
      "metadata": {
        "id": "u6V00ZDKiQ-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "community_size = graph.query(\n",
        "    \"\"\"\n",
        "MATCH (c:__Community__)<-[:IN_COMMUNITY*]-(e:__Entity__)\n",
        "WITH c, count(distinct e) AS entities\n",
        "RETURN split(c.id, '-')[0] AS level, entities\n",
        "\"\"\"\n",
        ")\n",
        "community_size_df = pd.DataFrame.from_records(community_size)\n",
        "percentiles_data = []\n",
        "for level in community_size_df[\"level\"].unique():\n",
        "    subset = community_size_df[community_size_df[\"level\"] == level][\"entities\"]\n",
        "    num_communities = len(subset)\n",
        "    percentiles = np.percentile(subset, [25, 50, 75, 90, 99])\n",
        "    percentiles_data.append(\n",
        "        [\n",
        "            level,\n",
        "            num_communities,\n",
        "            percentiles[0],\n",
        "            percentiles[1],\n",
        "            percentiles[2],\n",
        "            percentiles[3],\n",
        "            percentiles[4],\n",
        "            max(subset)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "# Create a DataFrame with the percentiles\n",
        "percentiles_df = pd.DataFrame(\n",
        "    percentiles_data,\n",
        "    columns=[\n",
        "        \"Level\",\n",
        "        \"Number of communities\",\n",
        "        \"25th Percentile\",\n",
        "        \"50th Percentile\",\n",
        "        \"75th Percentile\",\n",
        "        \"90th Percentile\",\n",
        "        \"99th Percentile\",\n",
        "        \"Max\"\n",
        "    ],\n",
        ")\n",
        "percentiles_df\n"
      ],
      "metadata": {
        "id": "sU2Eg54TLjAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the original implementation, communities on every level were summarized. In our case, that would be 8,590 communities and, consequently, 8,590 LLM calls. I would argue that depending on the hierarchical community structure, not every level needs to be summarized. For example, the difference between the last and the next-to-last level is only four communities (1,192 vs. 1,188). Therefore, we would be creating a lot of redundant summaries. One solution is to create an implementation that can make a single summary for communities on different levels that don’t change; another one would be to collapse community hierarchies that don’t change.\n",
        "\n",
        "Also, I am unsure if we want to summarize communities with only one member, as they might not provide much value or information. Here, we will summarize communities on levels 0, 1, and 4. First, we need to retrieve their information from the database:"
      ],
      "metadata": {
        "id": "FLE47rf9iYIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "community_info = graph.query(\"\"\"\n",
        "MATCH (c:`__Community__`)<-[:IN_COMMUNITY*]-(e:__Entity__)\n",
        "WHERE c.level IN [0,1,4]\n",
        "WITH c, collect(e ) AS nodes\n",
        "WHERE size(nodes) > 1\n",
        "CALL apoc.path.subgraphAll(nodes[0], {\n",
        "\twhitelistNodes:nodes\n",
        "})\n",
        "YIELD relationships\n",
        "RETURN c.id AS communityId,\n",
        "       [n in nodes | {id: n.id, description: n.description, type: [el in labels(n) WHERE el <> '__Entity__'][0]}] AS nodes,\n",
        "       [r in relationships | {start: startNode(r).id, type: type(r), end: endNode(r).id, description: r.description}] AS rels\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "A4NuLhY0mm3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "community_info[5]"
      ],
      "metadata": {
        "id": "Ur4JkHkLe3_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we need to prepare an LLM prompt that generates a natural language summarization based on the information provided by the elements of our community. We can take some inspiration from the [prompt the researchers used](https://github.com/microsoft/graphrag/blob/main/graphrag/prompt_tune/template/community_report_summarization.py).\n",
        "\n",
        "The authors not only summarized communities but also generated findings for each of them. A finding can be defined as concise information regarding a specific event or piece of information. One such example:\n",
        "```\n",
        "\"summary\": \"Abila City Park as the central location\",\n",
        "\"explanation\": \"Abila City Park is the central entity in this community, serving as the location for the POK rally. This park is the common link between all other\n",
        "entities, suggesting its significance in the community. The park's association with the rally could potentially lead to issues such as public disorder or conflict, depending on the\n",
        "nature of the rally and the reactions it provokes. [records: Entities (5), Relationships (37, 38, 39, 40)]\"\n",
        "```\n",
        "My intuition suggests that extracting findings with just a single pass might not be as comprehensive as we need, much like extracting entities and relationships.\n",
        "\n",
        "Furthermore, I have not found any references or examples of their use in their code in either local or global search retrievers. As a result, we’ll refrain from extracting findings in this instance. Or, as academics often put it: This exercise is left to the reader. Additionally, we have also skipped the claims or covariate information extraction, which looks similar to findings at first glance.\n",
        "\n",
        "The prompt we’ll use to produce the community summaries is fairly straightforward:"
      ],
      "metadata": {
        "id": "T_ybgPpwieQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "community_template = \"\"\"Based on the provided nodes and relationships that belong to the same graph community,\n",
        "generate a natural language summary of the provided information:\n",
        "{community_info}\n",
        "\n",
        "Summary:\"\"\"  # noqa: E501\n",
        "\n",
        "community_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Given an input triples, generate the information summary. No pre-amble.\",\n",
        "        ),\n",
        "        (\"human\", community_template),\n",
        "    ]\n",
        ")\n",
        "\n",
        "community_chain = community_prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "Tnc8aqN_e5M7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The only thing left is to turn community representations into strings to reduce the number of tokens by avoiding JSON token overhead and wrap the chain as a function:"
      ],
      "metadata": {
        "id": "S6PdeC7WisWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_string(data):\n",
        "    nodes_str = \"Nodes are:\\n\"\n",
        "    for node in data['nodes']:\n",
        "        node_id = node['id']\n",
        "        node_type = node['type']\n",
        "        if 'description' in node and node['description']:\n",
        "            node_description = f\", description: {node['description']}\"\n",
        "        else:\n",
        "            node_description = \"\"\n",
        "        nodes_str += f\"id: {node_id}, type: {node_type}{node_description}\\n\"\n",
        "\n",
        "    rels_str = \"Relationships are:\\n\"\n",
        "    for rel in data['rels']:\n",
        "        start = rel['start']\n",
        "        end = rel['end']\n",
        "        rel_type = rel['type']\n",
        "        if 'description' in rel and rel['description']:\n",
        "            description = f\", description: {rel['description']}\"\n",
        "        else:\n",
        "            description = \"\"\n",
        "        rels_str += f\"({start})-[:{rel_type}]->({end}){description}\\n\"\n",
        "\n",
        "    return nodes_str + \"\\n\" + rels_str\n",
        "\n",
        "def process_community(community):\n",
        "    stringify_info = prepare_string(community)\n",
        "    summary = community_chain.invoke({'community_info': stringify_info})\n",
        "    return {\"community\": community['communityId'], \"summary\": summary}"
      ],
      "metadata": {
        "id": "KdcKulkxVsXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prepare_string(community_info[3]))"
      ],
      "metadata": {
        "id": "F4RC8ydEWB0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can generate community summaries for the selected levels. Again, we parallelize calls for faster execution:"
      ],
      "metadata": {
        "id": "uNnO_edyiwa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = []\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    futures = {executor.submit(process_community, community): community for community in community_info}\n",
        "\n",
        "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing communities\"):\n",
        "        summaries.append(future.result())"
      ],
      "metadata": {
        "id": "5IERhJ3jgWtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One aspect I didn’t mention is that the authors also address the potential issue of exceeding context size when inputting community information. As graphs expand, the communities can grow significantly as well. In our case, the largest community comprised 545 members. Given that GPT-4o has a context size exceeding 100,000 tokens, we decided to skip this step.\n",
        "\n",
        "As our final step, we will store the community summaries back to the database:"
      ],
      "metadata": {
        "id": "sk2DEFEFi2ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store summaries\n",
        "graph.query(\"\"\"\n",
        "UNWIND $data AS row\n",
        "MERGE (c:__Community__ {id:row.community})\n",
        "SET c.summary = row.summary\n",
        "\"\"\", params={\"data\": summaries})"
      ],
      "metadata": {
        "id": "mKymAMJehSkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final graph structure:\n",
        "\n",
        "![title](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rkgiVH20AuG1X-rMzTipXQ.png)\n",
        "\n",
        "The graph now contains the original documents, extracted entities and relationships, as well as hierarchical community structure and summaries.\n",
        "\n",
        "Summary\n",
        "The authors of the “From Local to Global” paper have done a great job in demonstrating a new approach to GraphRAG. They show how we can combine and summarize information from various documents into a hierarchical knowledge graph structure.\n",
        "\n",
        "One thing that isn’t explicitly mentioned is that we can also integrate structured data sources in a graph; the input doesn’t have to be limited to unstructured text only.\n",
        "\n",
        "What I particularly appreciate about their extraction approach is that they capture descriptions for both nodes and relationships. Descriptions allow the LLM to retain more information than reducing everything to just node IDs and relationship types.\n",
        "\n",
        "Additionally, they demonstrate that a single extraction pass over the text might not capture all relevant information and introduce logic to perform multiple passes if necessary. The authors also present an interesting idea for performing summarizations over graph communities, allowing us to embed and index condensed topical information across multiple data sources.\n",
        "\n",
        "In the next blog post, we will go over the local and global search retriever implementations and talk about other approaches we could implement based on the given graph structure."
      ],
      "metadata": {
        "id": "xLX6Wrqai-sD"
      }
    }
  ]
}